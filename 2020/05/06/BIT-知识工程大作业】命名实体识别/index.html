<!DOCTYPE html>
<html lang="en">

<head>

  <!-- Minima -->
  <!-- Hexo theme created by @adisaktijrs -->

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">

  
  <title>【BIT 知识工程大作业】命名实体识别</title>
  
  <link rel="canonical" href="https://github.com/Narumi-Maria/Narumi-Maria.github.io/2020/05/06/BIT-%E7%9F%A5%E8%AF%86%E5%B7%A5%E7%A8%8B%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/">
  
  <meta name="description" content="一、实验简介从现代汉语切分、标注、注音语料库中用中文词向量表示，检测某词汇是否为团体机关单位名称（nt）。使用one hot编码和LSTM。  训练数据来源：https://klcl.pku.edu.cn/gxzy/231686.htm 简单来说，就是经过训练，根据前后文把下图中是nt的词找出来。 ">
  
  
  <meta name="author" content="John Doe">
  
  <meta property="og:image" content="https://github.com/Narumi-Maria/Narumi-Maria.github.ioundefined">
  
  <meta property="og:site_name" content="Hexo" />
  <meta property="og:type" content="article" />
  <meta property="og:title" content="【BIT 知识工程大作业】命名实体识别" />
  
  <meta property="og:description" content="一、实验简介从现代汉语切分、标注、注音语料库中用中文词向量表示，检测某词汇是否为团体机关单位名称（nt）。使用one hot编码和LSTM。  训练数据来源：https://klcl.pku.edu.cn/gxzy/231686.htm 简单来说，就是经过训练，根据前后文把下图中是nt的词找出来。 ">
  
  <meta property="og:url" content="https://github.com/Narumi-Maria/Narumi-Maria.github.io/2020/05/06/BIT-%E7%9F%A5%E8%AF%86%E5%B7%A5%E7%A8%8B%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/" />

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="【BIT 知识工程大作业】命名实体识别">
  
  <meta name="twitter:description" content="一、实验简介从现代汉语切分、标注、注音语料库中用中文词向量表示，检测某词汇是否为团体机关单位名称（nt）。使用one hot编码和LSTM。  训练数据来源：https://klcl.pku.edu.cn/gxzy/231686.htm 简单来说，就是经过训练，根据前后文把下图中是nt的词找出来。 ">
  
  
  <meta name="twitter:image" content="https://github.com/Narumi-Maria/Narumi-Maria.github.ioundefined">
  
  <meta name="twitter:url" content="https://github.com/Narumi-Maria/Narumi-Maria.github.io/2020/05/06/BIT-%E7%9F%A5%E8%AF%86%E5%B7%A5%E7%A8%8B%E5%A4%A7%E4%BD%9C%E4%B8%9A%E3%80%91%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/" />

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Preload fonts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="preload" href="../fonts/dm-serif-display-v4-latin-regular.woff2" as="font" type="font/woff2" crossorigin>
  <link rel="preload" href="../fonts/inter-v2-latin-regular.woff2" as="font" type="font/woff2" crossorigin>

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  
<link rel="stylesheet" href="/css/normalize.css">

  
<link rel="stylesheet" href="/css/skeleton.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
<link rel="stylesheet" href="/css/prism-dark.css">

  
<link rel="stylesheet" href="/css/prism-line-numbers.css">

  <!-- User css -->
  
  
<link rel="stylesheet" href="/css/user.css">

  

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="/images/favicon.png">

  <!-- Custom Theme Color Style
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <style>
  a:not(.icon) {
    text-decoration-color: #0FA0CE;
    background-image: linear-gradient(
      to bottom,
      rgba(0, 0, 0, 0) 50%,
      #0FA0CE 50%
    );
  }
  blockquote {
    border-left: 8px solid #0FA0CE;
  }
  .nanobar .bar {
    background: #0FA0CE;
  }
  .button.button-primary:hover,
  button.button-primary:hover,
  input[type="submit"].button-primary:hover,
  input[type="reset"].button-primary:hover,
  input[type="button"].button-primary:hover,
  .button.button-primary:focus,
  button.button-primary:focus,
  input[type="submit"].button-primary:focus,
  input[type="reset"].button-primary:focus,
  input[type="button"].button-primary:focus {
    background-color: #0FA0CE;
    border-color: #0FA0CE;
  }
  input[type="email"]:focus,
  input[type="number"]:focus,
  input[type="search"]:focus,
  input[type="text"]:focus,
  input[type="tel"]:focus,
  input[type="url"]:focus,
  input[type="password"]:focus,
  textarea:focus,
  select:focus {
    border: 1px solid #0FA0CE;
  }
</style>

  <!-- Google Analytics (With Privacy Settings On)
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  

<meta name="generator" content="Hexo 5.3.0"></head>

<body>
  <div class="container">
    <div class="row">
      <div>

        <div class="row">
  <div class="two columns" style="max-width: 50px">
    <h1 class="mt-2 mode">
      <div onclick=setDarkMode(true) id="darkBtn">🌑</div>
      <div onclick=setDarkMode(false) id="lightBtn" class=hidden>☀️</div>
      <script >
        if (localStorage.getItem('preferredTheme') == 'dark') {
          setDarkMode(true)
        }
        function setDarkMode(isDark) {
          var darkBtn = document.getElementById('darkBtn')
          var lightBtn = document.getElementById('lightBtn')
          if (isDark) {
            lightBtn.style.display = "block"
            darkBtn.style.display = "none"
            localStorage.setItem('preferredTheme', 'dark');
          } else {
            lightBtn.style.display = "none"
            darkBtn.style.display = "block"
            localStorage.removeItem('preferredTheme');
          }
          document.body.classList.toggle("darkmode");
        }
      </script>
    </h1>
  </div>

  <div class="six columns ml-1">
    <h1 class="mt-2">
      Hi Folks.
    </h1>
  </div>

  <div class="twelve columns">
    <div class="row">
      <div class="nine columns left">
        <a href="/">Home</a>
        
          
          <a href="/Works" class="ml">Works</a>
          
        
          
          <a href="/About" class="ml">About</a>
          
        
        
          
            <a href="mailto:test@test.test" target="_blank" class="ml">Email</a>
          
        
      </div>
    </div>
    <hr style="margin-bottom: 2.6rem">
  </div>
</div>

        <div class="trans">
            <h2>【BIT 知识工程大作业】命名实体识别</h2>

  <h3 id="一、实验简介"><a href="#一、实验简介" class="headerlink" title="一、实验简介"></a>一、实验简介</h3><p>从现代汉语切分、标注、注音语料库中用中文词向量表示，检测某词汇是否为团体机关单位名称（nt）。使用one hot编码和LSTM。 </p>
<p>训练数据来源：<a target="_blank" rel="noopener" href="https://klcl.pku.edu.cn/gxzy/231686.htm">https://klcl.pku.edu.cn/gxzy/231686.htm</a></p>
<p>简单来说，就是经过训练，根据前后文把下图中是nt的词找出来。<br><img src="/Narumi-Maria.github.io/images/pasted-38.png" alt="upload successful"></p>
<h3 id="二、实验步骤及代码详解"><a href="#二、实验步骤及代码详解" class="headerlink" title="二、实验步骤及代码详解"></a>二、实验步骤及代码详解</h3><p>1.“vec2json2dic.py”用于将词向量的中文表示，即“ctb.50d.vec”转化成词向量字典 </p>
<ul>
<li>通 过 别 的 模 块 调 用 ： import vec2json2dic 和 vec2json2dic.dumpjson(‘ctb.50d.vec’, ‘ctb50d.json’)则生成文件 vec.json。默认 i=0，将字典保存到本地，若 i=1，则字典不保存， 直接返回。</li>
<li>通过别的模块调用：import vec2json2dic 和 vec2json2dic.loadjson(‘ctb50d.json’)则 dic 为词向量字典，格式为： {‘我’:[1,2,3,4,5], ‘你’:[1,2,3,4,5], }</li>
</ul>
<p>2.“totenser.py”用于构造实验所需的向量 </p>
<ul>
<li><p>函数 xy2list 得到词和标签的列表。它的输入为需要切分的文本名称。我用 txt = [x for x in f.read().split(‘\n’) if len(x) != 0]将每行作为一句话分开进行训练。接下来对每一句话进行和 上一次作业一样的处理，将词和标签分开，是 nt 记为 [0,1,0]（ppt 中的 B)，不是 nt 记 为 [1,0,0](ppt 中的 O)。对带中括号中的元素做特殊处理，如果中括号的后面是‘nt’，那 么中括号中的第一个词的标签记为 [0,1,0]（ppt 中的 B)，其余的单词记为 [0,0,1]（ppt 中 的 I）。将每一句话的所有词和标签存在 wordlist 中，再将每一句话的词列表 wordlist 添加 到一个大的列表 text 中。对标签也是以句子为单位，进行同样的处理，最后得到一个由每 句话的每个词的标签表示的列表组成的列表 ys。 （注：后续编写中我发现其实把标签设为一维更方便，但是由于选修四门复习吐血没有修改） </p>
</li>
<li><p>函数 getDir 查找每一个词在词表中的向量表示，并返回该词的向量表示。函数 wordList2Dirlist 用于返回每一句话组成的词向量列表和每一句话的词数。这两个函数都是函 数 make_tensors 的辅助函数。 </p>
</li>
<li><p>函数 make_tensors 用于构造词与标签的 tensor。首先引用 vec2json2dic 中的 loadjson 函数生成词向量字典 wordDir。接着调用函数 getDir 和函数 wordList2Dirlist，构造出三个 tensor，分别为 seq_tensor（形状为句子数<em>最长句子词数</em>50 维，存放所有句子的词向量表 示）、seq_length（形状为句子数<em>1，存放每个句子长度）、y_tensor(形状为句子数</em>最长 句子词数*3 维，存放所有句子的词标签表示）。接下来将这三个 tensor 按照句子长度从大 到小的顺序进行排序（为后续类中 packedsequence object 的生成所用） </p>
</li>
</ul>
<p>3.“classifier.py”用于构建类</p>
<ul>
<li><code>__Intial__</code>函数构造这个神经网络的形状，给类中的参数赋值（Input：输入词向量的维 度，hidden_size:h、c 的维度(128)，batch_size：批量大小，n_layers 从 x 到 h 的层数，bidirectional: 是否双向 RNN，batch_first：batch_size 是否在第一维度）。接着定义 LSTM 层和线性层 Linear。</li>
<li>forward 函数表示前向传播的这一过程。由于输入的 embedding 是将句子长度补齐（用0 补）的 tensor，所以这里将它压缩变换成 LSTM 可以接受的 packedsequence object（这也 是为什么之前要对句子进行长度排序的原因）。然后将 x 输入到 LSTM 中，得到的 output 是个元组，第一维是后面填充 0 的 tensor，第二个是每句话的长度的列表。将 output 的第 一维解压，并经过线性层变成预测矩阵 y_pre，再将 output 第二维，即句子长度列表和 y_pre 一起返回。</li>
</ul>
<p>4.“lesson3.py”为训练主程序 </p>
<ul>
<li>初始化超参数 lr（学习率）、num（迭代次数）、batchsize（批量大小）</li>
<li>调用函数 load（函数 load 中调用 totenser.py 中的函数），加载出上文提到的三个向量 seq_tensor、seq_length、y_tensor。</li>
<li>根据 batchsize 大小将 seq_tensor、seq_length、y_tensor 分批。 + 初始化模型 cla。取 h、c 为 128 维，x 到 h 经过两层神经网络，双向 RNN。</li>
<li>下一步得到模型的权重（如果之前有训练好的权重可以加载出来累计上一次训练）， 否则直接跳到下一步。</li>
<li>调用训练函数 train，输入模型，学习率，迭代次数和上文提到的三个 tensor，seq_tensor、 seq_length、y_tensor。选用 pytorch 库中的交叉熵损失函数 CrossEntropyLoss，优化方式选用 adam。将 y_tensor 变成句子数（batchsize）<em>最长句子词数</em>1 维（0/1/2），用于计算损失 函数。将每一句话的预测结果和实际值放入交叉熵损失函数中，进行计算后累加到 loss 上， 每一批优化一次参数，每迭代完一轮打印一次 loss 值，每 5 轮跌倒保存一次模型权重，防止 由于电脑宕机造成的白学。 </li>
<li>将训练好的模型作用到验证集。首先调用函数 load，输入“yanzheng2”为文件名，获 得三个 tensor：seq_tensor、seq_length、y_tensor。接下来加载模型权重，最后将模型、tensor 输入到预测函数 predict 中，在 predict 中用模型训练出 y_pre，再将 y_pre 和 y_tensor 从 3 维转为 1 维（词数*1），以便于计算 f1。 </li>
<li>计算 f1。函数 f1 用一维的 Y 矩阵计算 f1。首先先由 X、w 算出预测矩阵（3<em>词数）， 再用 torch.argmax 函数找到每个词预测为 B、I、O 的概率中最大的那个，返回一个词数</em>1 的矩阵 Y_pre（预测值）。 将 Y_pre 和 Y（实际值）放入函数 location(t)中，得到由每个 实体位置所构成的两个列表，然 后对比两个列表中的值，一样的则计数到 TP 中去。而两 个列表中的元素总数分别为预测为真的数量和实际为真的数量，由这些值即可计算 f1。</li>
</ul>
<h3 id="三、超参数分析："><a href="#三、超参数分析：" class="headerlink" title="三、超参数分析："></a>三、超参数分析：</h3><p><img src="/Narumi-Maria.github.io/images/pasted-33.png" alt="upload successful"></p>
<p>从表格可以看出，在 0.01 的学习率下将整个训练集跑 30 遍模型效果最好。 损失函数下降曲线图：</p>
<p><img src="/Narumi-Maria.github.io/images/pasted-34.png" alt="upload successful"><br>Lr=0.1 时，由于损失函数在迭代过程中发散，所以我提前终止了程序，没有计算 f</p>
<p><img src="/Narumi-Maria.github.io/images/pasted-35.png" alt="upload successful"><br>分析这些数据可以看出，当学习率较小时，模型收敛速度慢。当学习率较大时，损失函 数会出现发散现象。当学习率合适时，过多的迭代步数，虽然损失函数持续下降，但是验证 集 f1 不升反降，说明出现了过拟合现象，应该停止迭代。<br>下面是我用最好学习率 0.01 和最好的迭代次数 30 对 hidden_size 和 n_layers 进行的分析：</p>
<p><img src="/Narumi-Maria.github.io/images/pasted-36.png" alt="upload successful"><br>从上述实验数据可以看出，h=128 和 h=64 对实验结果影响不大，略大的 h 模型效果会 略好一些，但是区别不是很明显，猜测是因为 x 的维度并不大。同时，h 越大训练的时间越 长，h=128 时，20 分钟一轮，h=64 约 10 分钟一轮，h=32 约 5 分钟一轮。私以为以与词向 量长度（50）差不多的 32 是比较合适的。 之前我的猜想是层数越多模型效果越好，但现在看来并非是如此，n_layer=3 的效果远 不如 n_layer=2。或许是更多的层数 30 次训练不够。 最后选用最好的一组参数，即 hidden_size=128，n_layers=2，lr=0.01，迭代 30 次得 到的模型作用于测试集上，得到的结果如下图所示：</p>
<p><img src="/Narumi-Maria.github.io/images/pasted-37.png" alt="upload successful"></p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;</span><span class="string">&#x27;vec2json2dic.py&#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">import json</span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line">def loadjson(filename):</span><br><span class="line">    with open(filename,<span class="string">&#x27;r&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)as f:</span><br><span class="line">        dic=json.loads(f.read())</span><br><span class="line">    <span class="built_in">return</span> dic</span><br><span class="line"></span><br><span class="line">def dumpjson(filename,name,i=0):</span><br><span class="line">    with open(filename,<span class="string">&#x27;r&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)as f:</span><br><span class="line">        lines=f.read().splitlines()</span><br><span class="line">        vec=&#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> lines:</span><br><span class="line">            list=j.split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">            vec[list[0]]=[x <span class="keyword">for</span> x <span class="keyword">in</span> list[1:] <span class="keyword">if</span> re.search(r<span class="string">&#x27;\d&#x27;</span>,x)]<span class="comment">#re.search(r&#x27;\d&#x27;,x)若x中有0~9的数字就返回为真</span></span><br><span class="line">    <span class="keyword">if</span> i!=0:</span><br><span class="line">        <span class="built_in">return</span> vec</span><br><span class="line">    with open(name,<span class="string">&#x27;w&#x27;</span>,encoding=<span class="string">&#x27;utf-8&#x27;</span>)as f:</span><br><span class="line">        f.write(json.dumps(vec))</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;</span><span class="string">&#x27;</span></span><br><span class="line"><span class="string">通过别的模块调用：</span></span><br><span class="line"><span class="string">    import vec2json2dic</span></span><br><span class="line"><span class="string">    vec2json2dic.dumpjson(&#x27;</span>ctb.50d.vec<span class="string">&#x27;,&#x27;</span>ctb50d.json<span class="string">&#x27;)</span></span><br><span class="line"><span class="string">则生成文件vec.json</span></span><br><span class="line"><span class="string">默认i=0，将字典保存到本地，若i=1，则字典不保存，直接返回</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">通过别的模块调用：</span></span><br><span class="line"><span class="string">    import vec2json2dic</span></span><br><span class="line"><span class="string">    vec2json2dic.loadjson(&#x27;</span>ctb50d.json<span class="string">&#x27;)</span></span><br><span class="line"><span class="string">则dic为词向量字典，格式为：</span></span><br><span class="line"><span class="string">    &#123;</span></span><br><span class="line"><span class="string">        &#x27;</span>我<span class="string">&#x27;:[1,2,3,4,5],</span></span><br><span class="line"><span class="string">        &#x27;</span>你<span class="string">&#x27;:[1,2,3,4,5],</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">&#x27;</span><span class="string">&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;</span><span class="string">&#x27;totensor.py&#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">import re</span><br><span class="line">import torch</span><br><span class="line">import vec2json2dic</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查找每一个词在词表中的向量表示</span></span><br><span class="line">def getDir(word, wordDir):</span><br><span class="line">    results = wordDir.get(word, wordDir[<span class="string">&#x27;-unknown-&#x27;</span>])</span><br><span class="line">    results = list(map(<span class="built_in">float</span>, results))  <span class="comment"># 格式转换，map把str转为float，list把迭代器转为列表</span></span><br><span class="line">    <span class="built_in">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回每一句话的词组成的向量列表和每一句话的词数</span></span><br><span class="line">def wordList2Dirlist(wordlist, wordDir):</span><br><span class="line">    arr = [getDir(word, wordDir) <span class="keyword">for</span> word <span class="keyword">in</span> wordlist]</span><br><span class="line">    <span class="built_in">return</span> arr, len(arr)</span><br><span class="line"></span><br><span class="line"><span class="comment">#构造词与标签的tensor</span></span><br><span class="line">def make_tensors(text, ys):</span><br><span class="line">    wordDir = vec2json2dic.loadjson(<span class="string">&#x27;ctb50d.json&#x27;</span>)</span><br><span class="line">    sequences_and_lengths = [wordList2Dirlist(sentence, wordDir) <span class="keyword">for</span> sentence <span class="keyword">in</span> text]  <span class="comment"># 由每句话的词向量组和句子长度构成的元组</span></span><br><span class="line">    name_sequences = [s1[0] <span class="keyword">for</span> s1 <span class="keyword">in</span> sequences_and_lengths]  <span class="comment"># 每个句子的的词向量组组成的列表</span></span><br><span class="line">    seq_lengths = [s1[1] <span class="keyword">for</span> s1 <span class="keyword">in</span> sequences_and_lengths]  <span class="comment"># 每个句子的长度组成的列表</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构造词的tensor</span></span><br><span class="line">    seq_tensor = torch.zeros(len(name_sequences), max(seq_lengths), 50)</span><br><span class="line">    <span class="keyword">for</span> idx, (seq, seq_len) <span class="keyword">in</span> enumerate(zip(name_sequences, seq_lengths), 0):  <span class="comment"># 遍历每一句话</span></span><br><span class="line">        seq_tensor[idx, :seq_len] = torch.tensor(seq) <span class="comment"># 把一个句子中的词的向量表示批量复制</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构造标签的tensor</span></span><br><span class="line">    y_tensor = torch.zeros(len(name_sequences), max(seq_lengths), 3)</span><br><span class="line">    <span class="keyword">for</span> idx, (seq, seq_len) <span class="keyword">in</span> enumerate(zip(ys, seq_lengths), 0):  <span class="comment"># 遍历每一句话</span></span><br><span class="line">        y_tensor[idx, :seq_len] = torch.tensor(seq)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 从大到小排序</span></span><br><span class="line">    seq_lengths=torch.tensor(seq_lengths)</span><br><span class="line">    seq_lengths,perm_id=seq_lengths.sort(dim=0,descending=True)</span><br><span class="line">    seq_tensor=seq_tensor[perm_id]</span><br><span class="line">    y_tensor=y_tensor[perm_id]</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> seq_tensor,seq_lengths,y_tensor</span><br><span class="line"></span><br><span class="line"><span class="comment">#构造文本由每个句子组成的列表text和对应的每个句子组成的标签列表ys</span></span><br><span class="line">def xy2list(file):</span><br><span class="line">    with open(file, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) as f:</span><br><span class="line">        txt = [x <span class="keyword">for</span> x <span class="keyword">in</span> f.read().split(<span class="string">&#x27;\n&#x27;</span>) <span class="keyword">if</span> len(x) != 0]</span><br><span class="line"></span><br><span class="line">        text = []</span><br><span class="line">        ys = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> sentence <span class="keyword">in</span> txt:</span><br><span class="line">            wordlist1 = re.split(r<span class="string">&quot;  (?![^\[]*\])&quot;</span>, sentence)</span><br><span class="line"></span><br><span class="line">            wordlist = []</span><br><span class="line">            ylist = []</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> word1 <span class="keyword">in</span> wordlist1:</span><br><span class="line">                <span class="keyword">if</span> word1 == <span class="string">&#x27;&#x27;</span> or word1 == <span class="string">&#x27;\n&#x27;</span>:</span><br><span class="line">                    <span class="built_in">continue</span></span><br><span class="line">                <span class="comment"># 处理[]中的词汇</span></span><br><span class="line">                <span class="keyword">if</span> <span class="string">&#x27;[&#x27;</span> <span class="keyword">in</span> word1:  <span class="comment"># [国家/n  环保局/n]nt</span></span><br><span class="line">                    templist = word1.split(<span class="string">&#x27;]&#x27;</span>)  <span class="comment"># [国家/n  环保局/n       nt</span></span><br><span class="line">                    temp = templist[0].replace(<span class="string">&#x27;[&#x27;</span>, <span class="string">&#x27;&#x27;</span>)  <span class="comment"># 国家/n  环保局/n</span></span><br><span class="line">                    templist2 = temp.split(<span class="string">&#x27;  &#x27;</span>)  <span class="comment"># 国家/n      环保局/n</span></span><br><span class="line">                    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(templist2)):</span><br><span class="line">                        templist3 = templist2[i].split(<span class="string">&#x27;/&#x27;</span>)</span><br><span class="line">                        wordlist.append(templist3[0].replace(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>))</span><br><span class="line">                        <span class="keyword">if</span> i == 0 and (word1[-2:] == <span class="string">&#x27;nt&#x27;</span> or word1[-2:] == <span class="string">&#x27;nt\n&#x27;</span>):  <span class="comment"># []nt中的第一个</span></span><br><span class="line">                            ylist.append([0, 1, 0])  <span class="comment"># B</span></span><br><span class="line">                        <span class="keyword">elif</span> i != 0 and (word1[-2:] == <span class="string">&#x27;nt&#x27;</span> or word1[-2:] == <span class="string">&#x27;nt\n&#x27;</span>):  <span class="comment"># []nt中的后面几个</span></span><br><span class="line">                            ylist.append([0, 0, 1])  <span class="comment"># I</span></span><br><span class="line">                        <span class="keyword">else</span>:  <span class="comment"># []xx</span></span><br><span class="line">                            ylist.append([1, 0, 0])  <span class="comment"># O</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    templist = word1.split(<span class="string">&#x27;/&#x27;</span>)</span><br><span class="line">                    wordlist.append(templist[0].replace(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>))</span><br><span class="line">                    <span class="keyword">if</span> word1[-2:] == <span class="string">&#x27;nt&#x27;</span>:</span><br><span class="line">                        ylist.append([0, 1, 0])  <span class="comment"># B</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        ylist.append([1, 0, 0])  <span class="comment"># O</span></span><br><span class="line"></span><br><span class="line">            text.append(wordlist)</span><br><span class="line">            ys.append(ylist)</span><br><span class="line">        <span class="built_in">return</span> text, ys</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;</span><span class="string">&#x27;classifier.py&#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">import torch.nn</span><br><span class="line"></span><br><span class="line">class classifier(torch.nn.Module):</span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    def __init__(self, input_size, hidden_size, batch_size, n_layers=2, bidirectional=True, batch_first=True,cuda=False):</span><br><span class="line">        <span class="comment"># hidden_size:h的维度(128)，batch_size：批量大小，n_layers从x到h的层数，bidirectional:是否双向RNN，batch_first：batch_size是否在第一维度</span></span><br><span class="line">        super(classifier, self).__init__()  <span class="comment"># 调用父类的构造方法</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 给类中的参数赋值</span></span><br><span class="line">        self.batch_first = batch_first</span><br><span class="line">        self.n_directions = 2 <span class="keyword">if</span> bidirectional <span class="keyword">else</span> 1  <span class="comment"># n_directions：传播方向数</span></span><br><span class="line">        <span class="comment">#self.h0 = torch.zeros(n_layers * self.n_directions, batch_size, hidden_size)  # feature h</span></span><br><span class="line">        <span class="comment">#self.c0 = torch.zeros(n_layers * self.n_directions, batch_size, hidden_size)  # memory c</span></span><br><span class="line">        self.h0 = torch.nn.Parameter(torch.zeros(n_layers * self.n_directions, batch_size, hidden_size))  <span class="comment"># feature h</span></span><br><span class="line">        self.c0 = torch.nn.Parameter(torch.zeros(n_layers * self.n_directions, batch_size, hidden_size))  <span class="comment"># memory c</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">#调用GPU</span></span><br><span class="line">        <span class="keyword">if</span> cuda:</span><br><span class="line">            self.h0=self.h0.cuda()</span><br><span class="line">            self.c0=self.c0.cuda()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 构造LSTM层和线性层</span></span><br><span class="line">        self.LSTM = torch.nn.LSTM(input_size, hidden_size, n_layers, bidirectional=bidirectional,</span><br><span class="line">                                  batch_first=batch_first)</span><br><span class="line">        self.Linear = torch.nn.Linear(hidden_size * self.n_directions, 3)  <span class="comment"># 转化成3维</span></span><br><span class="line"></span><br><span class="line">    def forward(self, embedding, seq_lengths):</span><br><span class="line">        <span class="string">&#x27;&#x27;</span><span class="string">&#x27;</span></span><br><span class="line"><span class="string">        embedding形状为batch_size*seq_lengths*50</span></span><br><span class="line"><span class="string">        seq_lengths是一个存每个句子长度的列表</span></span><br><span class="line"><span class="string">        由于每句话句子长度不一样，为了不影响 LSTM 的发挥，增加效率，用pack_padded去掉0</span></span><br><span class="line"><span class="string">        之前把句子从长到短排序就是为了方便使用pack_padded</span></span><br><span class="line"><span class="string">        x为PackedSequence object，是 LSTM 可接受的一个类型</span></span><br><span class="line"><span class="string">        &#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">        x = torch.nn.utils.rnn.pack_padded_sequence(embedding, seq_lengths, batch_first=self.batch_first)</span><br><span class="line">        <span class="comment">#x=embedding</span></span><br><span class="line">        <span class="string">&#x27;&#x27;</span><span class="string">&#x27;</span></span><br><span class="line"><span class="string">        依据格式使用LSTM</span></span><br><span class="line"><span class="string">        output是个元组，第一维是后面要填充0的tensor，第二个是每句话的长度的列表</span></span><br><span class="line"><span class="string">        hidden里面是hn和cn</span></span><br><span class="line"><span class="string">        &#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">        output, hidden = self.LSTM(x, (self.h0, self.c0))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 再把0补上变回tensor</span></span><br><span class="line">        output = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, padding_value=0, total_length=None)</span><br><span class="line">        y_pre = self.Linear(output[0])</span><br><span class="line">        <span class="built_in">return</span> y_pre, output[1]</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;</span><span class="string">&#x27;lesson3.py&#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">import torch.nn</span><br><span class="line">import numpy as np</span><br><span class="line">from classifier import classifier</span><br><span class="line">from totensor import *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#加载需要的向量</span></span><br><span class="line">def load(data):</span><br><span class="line">    try:</span><br><span class="line">        seq_tensor = torch.load(<span class="string">&#x27;pt/&#x27;</span> + data + <span class="string">&#x27;_seq_tensor.pt&#x27;</span>)</span><br><span class="line">        seq_lengths = torch.load(<span class="string">&#x27;pt/&#x27;</span> + data + <span class="string">&#x27;_seq_lengths.pt&#x27;</span>)</span><br><span class="line">        y_tensor = torch.load(<span class="string">&#x27;pt/&#x27;</span> + data + <span class="string">&#x27;_y_tensor.pt&#x27;</span>)</span><br><span class="line">    except:</span><br><span class="line">        text, ys = xy2list(data + <span class="string">&#x27;.txt&#x27;</span>)</span><br><span class="line">        seq_tensor, seq_lengths, y_tensor = make_tensors(text, ys)</span><br><span class="line">        torch.save(seq_tensor, <span class="string">&#x27;pt/&#x27;</span> + data + <span class="string">&#x27;_seq_tensor.pt&#x27;</span>)</span><br><span class="line">        torch.save(seq_lengths, <span class="string">&#x27;pt/&#x27;</span> + data + <span class="string">&#x27;_seq_lengths,pt&#x27;</span>)</span><br><span class="line">        torch.save(y_tensor, <span class="string">&#x27;pt/&#x27;</span> + data + <span class="string">&#x27;_y_tensor.pt&#x27;</span>)</span><br><span class="line">    <span class="built_in">return</span> seq_tensor,seq_lengths,y_tensor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#定位函数，用于辅助计算f1</span></span><br><span class="line">def location(t):</span><br><span class="line">    start = -1</span><br><span class="line">    list = []</span><br><span class="line">    <span class="keyword">for</span> n, i <span class="keyword">in</span> enumerate(t):</span><br><span class="line">        <span class="keyword">if</span> i.item() == 1:  <span class="comment"># 1=&gt;B 0=&gt;O 2=&gt;I</span></span><br><span class="line">            <span class="keyword">if</span> start != -1:</span><br><span class="line">                tup = (start, n)</span><br><span class="line">                list.append(tup)</span><br><span class="line">                start = -1</span><br><span class="line">            start = n</span><br><span class="line">        <span class="keyword">elif</span> start != -1 and i.item() == 0:</span><br><span class="line">            tup = (start, n)</span><br><span class="line">            list.append(tup)</span><br><span class="line">            start = -1</span><br><span class="line">    <span class="built_in">return</span> list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#计算f1</span></span><br><span class="line">def f1(Y_pre, Y):</span><br><span class="line">    s_pre = location(Y_pre)</span><br><span class="line">    s_true = location(Y)</span><br><span class="line">    TP = [x <span class="keyword">for</span> x <span class="keyword">in</span> s_pre <span class="keyword">if</span> x <span class="keyword">in</span> s_true]</span><br><span class="line">    TP = len(TP)</span><br><span class="line">    s_true = len(s_true)</span><br><span class="line">    s_pre = len(s_pre)</span><br><span class="line">    <span class="keyword">if</span> s_pre:</span><br><span class="line">        P = TP / s_pre</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        P=0</span><br><span class="line">    R = TP / s_true</span><br><span class="line">    <span class="keyword">if</span> (P + R):</span><br><span class="line">        F1 = 2 * P * R / (P + R)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        F1=0</span><br><span class="line">    <span class="built_in">return</span> F1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def predict(cla,seq_tensor,seq_lengths,y_tensor):</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        y_pre,lengths=cla(seq_tensor,seq_lengths)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#预测矩阵和实际矩阵从句子数*句子词数*3维转成总词数*一维（0，1，2）便于计算f1</span></span><br><span class="line">    y_pre=y_pre.view(-1,3)</span><br><span class="line">    y_pre=torch.argmax(y_pre,dim=1)</span><br><span class="line">    y_tensor=y_tensor.view(-1,3)</span><br><span class="line">    y_tensor=torch.argmax(y_tensor,dim=1)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(f1(y_pre,y_tensor))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def find_valid_f1(lr):<span class="comment"># 求验证集f1</span></span><br><span class="line">    cuda = False</span><br><span class="line">    </span><br><span class="line">    seq_tensor, seq_lengths, y_tensor = load(<span class="string">&quot;yanzheng2&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> cuda:  <span class="comment"># 调用gpu</span></span><br><span class="line">        seq_tensor = seq_tensor.cuda()</span><br><span class="line">        seq_lengths = seq_lengths.cuda()</span><br><span class="line">        y_tensor = y_tensor.cuda()</span><br><span class="line">    </span><br><span class="line">    cla = classifier(50, 32, n_layers=2, batch_size=seq_tensor.shape[0], bidirectional=True, batch_first=True)</span><br><span class="line">    try:</span><br><span class="line">        state_dict = torch.load(<span class="string">&quot;weight/&quot;</span> + str(lr) + <span class="string">&quot;.pth&quot;</span>)</span><br><span class="line">        cla.load_state_dict(state_dict)</span><br><span class="line">    except:</span><br><span class="line">        pass</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;valid_f1:&quot;</span>)</span><br><span class="line">    predict(cla, seq_tensor, seq_lengths, y_tensor)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def train(cla,seq_tensor,seq_lengths,y_tensor,lr,num,cuda=False):</span><br><span class="line">    criterion=torch.nn.CrossEntropyLoss() <span class="comment"># 交叉熵损失函数</span></span><br><span class="line">    optimizer=torch.optim.Adam(cla.parameters(),lr=lr)</span><br><span class="line">    <span class="comment">#把y_tensor变成句子数（batch——size）*最长句子词数*1维，便于交叉熵</span></span><br><span class="line">    <span class="comment">#y_tensor=torch.tensor(np.array([torch.argmax(x,dim=2) for x in y_tensor]))</span></span><br><span class="line">    y_tensor = [torch.argmax(x, dim=2) <span class="keyword">for</span> x <span class="keyword">in</span> y_tensor]</span><br><span class="line">    <span class="comment">#y_tensor =torch.tensor(y_tensor)</span></span><br><span class="line"></span><br><span class="line">    costs=[]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(num):  <span class="comment">#每个批次分别下降</span></span><br><span class="line">        cost = 0</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(len(seq_tensor)-1):  <span class="comment">#去掉最后一组</span></span><br><span class="line">            y_pre,lengths=cla(seq_tensor[k],seq_lengths[k])</span><br><span class="line">            <span class="comment">#loss=criterion(y_pre,y_tensor)</span></span><br><span class="line">            <span class="keyword">if</span> cuda:</span><br><span class="line">                loss = torch.Tensor([0]).cuda()  <span class="comment"># 注意大写！！！</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                loss = torch.Tensor([0])</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(len(lengths)):</span><br><span class="line">                loss+=criterion(y_pre[i,:lengths[i]],y_tensor[k][i,:lengths[i]])</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line">            cost+=loss.item()</span><br><span class="line">        <span class="comment">#print(&quot;epoch=&#123;&#125;,loss=&#123;&#125;&quot;.format(j,loss.item()))</span></span><br><span class="line">        cost /= len(seq_tensor) - 1  <span class="comment">#计算整体的损失函数值</span></span><br><span class="line">        costs.append(cost)</span><br><span class="line">        <span class="keyword">if</span> j%5==0: <span class="comment"># 每5步保存一次，防止电脑突然宕机造成的白学</span></span><br><span class="line">            torch.save(cla.state_dict(),<span class="string">&quot;weight/&quot;</span>+str(lr)+<span class="string">&quot;.pth&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> j%10==0: <span class="comment"># 每10步算一次f1</span></span><br><span class="line">            find_valid_f1(lr)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">return</span> costs</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&quot;__main__&quot;</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment">#超参数</span></span><br><span class="line">    lr=0.01</span><br><span class="line">    num=30</span><br><span class="line">    batchsize=100</span><br><span class="line">    cuda=False</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载数据</span></span><br><span class="line">    seq_tensor,seq_lengths,y_tensor=load(<span class="string">&quot;xunlian1&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#调用GPU</span></span><br><span class="line">    <span class="keyword">if</span> cuda:</span><br><span class="line">        seq_tensor=seq_tensor.cuda()</span><br><span class="line">        seq_lengths=seq_lengths.cuda()</span><br><span class="line">        y_tensor=y_tensor.cuda()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#根据batchsize分批</span></span><br><span class="line">    seq_tensor=[x <span class="keyword">for</span> x <span class="keyword">in</span> seq_tensor.split(batchsize,0)]</span><br><span class="line">    seq_lengths=[x <span class="keyword">for</span> x <span class="keyword">in</span> seq_lengths.split(batchsize,0)]</span><br><span class="line">    y_tensor=[x <span class="keyword">for</span> x <span class="keyword">in</span> y_tensor.split(batchsize,0)]</span><br><span class="line"></span><br><span class="line">    <span class="comment">#初始化模型</span></span><br><span class="line">    cla=classifier(50,32,n_layers=2,batch_size=batchsize,bidirectional=True,batch_first=True,cuda=cuda)</span><br><span class="line">    <span class="keyword">if</span> cuda:</span><br><span class="line">        cla.cuda()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#加载权重</span></span><br><span class="line">    try:</span><br><span class="line">        state_dict=torch.load(<span class="string">&quot;weight/&quot;</span>+str(lr)+<span class="string">&quot;.pth&quot;</span>)</span><br><span class="line">        cla.load_state_dict(state_dict)</span><br><span class="line">    except:</span><br><span class="line">        pass</span><br><span class="line"></span><br><span class="line">    <span class="comment">#训练</span></span><br><span class="line">    costs=train(cla,seq_tensor,seq_lengths,y_tensor,lr,num,cuda=cuda)</span><br><span class="line">    <span class="comment">#predict(cla,seq_tensor,seq_lengths,y_tensor)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#画图</span></span><br><span class="line">    costs = np.squeeze(costs)</span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;cost&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;iterations (per epoch)&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&quot;(learning_rate,num_iterations) : &quot;</span> + str(lr)+<span class="string">&#x27; &#x27;</span>+str(num))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    find_valid_f1(0.01)</span><br></pre></td></tr></table></figure>
  <p> — May 6, 2020</p>
  


          <div class="row mt-2">
  
    <div class="eight columns">
      <p id="madewith">Made with ❤ and
        <a class="footer-link icon" href="https://hexo.io" target="_blank" style="text-decoration: none;" rel="noreferrer" aria-label="Hexo.io">
        <svg class="hexo svg-hov" width="14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><title>Hexo.js</title><path d="M12 .007L1.57 6.056V18.05L12 23.995l10.43-6.049V5.952L12 .007zm4.798 17.105l-.939.521-.939-.521V12.94H9.08v4.172l-.94.521-.938-.521V6.89l.939-.521.939.521v4.172h5.84V6.89l.94-.521.938.521v10.222z"/></svg>
        </a>
        
        at <a href="https://en.wikipedia.org/wiki/Earth" target="_blank" rel="noreferrer">Earth</a>.</p>
        
    </div>

    <!-- Sepcial thanks to https://simpleicons.org/ for the icons -->
    <div class="four columns mb-3 posisi" >
      
      <a class="ml-0 footer-link icon" href="https://github.com/adisaktijrs" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="GitHub">
        <svg class="github svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
      </a>
      

      
      <a class="ml-0 footer-link icon" href="https://linkedin.com/in/adisaktijrs" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="LinkedIn">
        <svg class="linkedin svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>LinkedIn</title><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/></svg>
      </a>
      

      
      <a class="ml-0 footer-link icon" href="https://twitter.com/adisaktijrs" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="Twitter">
        <svg class="twitter svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Twitter</title><path d="M23.954 4.569c-.885.389-1.83.654-2.825.775 1.014-.611 1.794-1.574 2.163-2.723-.951.555-2.005.959-3.127 1.184-.896-.959-2.173-1.559-3.591-1.559-2.717 0-4.92 2.203-4.92 4.917 0 .39.045.765.127 1.124C7.691 8.094 4.066 6.13 1.64 3.161c-.427.722-.666 1.561-.666 2.475 0 1.71.87 3.213 2.188 4.096-.807-.026-1.566-.248-2.228-.616v.061c0 2.385 1.693 4.374 3.946 4.827-.413.111-.849.171-1.296.171-.314 0-.615-.03-.916-.086.631 1.953 2.445 3.377 4.604 3.417-1.68 1.319-3.809 2.105-6.102 2.105-.39 0-.779-.023-1.17-.067 2.189 1.394 4.768 2.209 7.557 2.209 9.054 0 13.999-7.496 13.999-13.986 0-.209 0-.42-.015-.63.961-.689 1.8-1.56 2.46-2.548l-.047-.02z"/></svg>
      </a>
      

      
      <a class="ml-0 footer-link icon" href="https://instagram.com/adisaktijrs" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="Instagram">
        <svg class="instagram svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Instagram</title><path d="M12 0C8.74 0 8.333.015 7.053.072 5.775.132 4.905.333 4.14.63c-.789.306-1.459.717-2.126 1.384S.935 3.35.63 4.14C.333 4.905.131 5.775.072 7.053.012 8.333 0 8.74 0 12s.015 3.667.072 4.947c.06 1.277.261 2.148.558 2.913.306.788.717 1.459 1.384 2.126.667.666 1.336 1.079 2.126 1.384.766.296 1.636.499 2.913.558C8.333 23.988 8.74 24 12 24s3.667-.015 4.947-.072c1.277-.06 2.148-.262 2.913-.558.788-.306 1.459-.718 2.126-1.384.666-.667 1.079-1.335 1.384-2.126.296-.765.499-1.636.558-2.913.06-1.28.072-1.687.072-4.947s-.015-3.667-.072-4.947c-.06-1.277-.262-2.149-.558-2.913-.306-.789-.718-1.459-1.384-2.126C21.319 1.347 20.651.935 19.86.63c-.765-.297-1.636-.499-2.913-.558C15.667.012 15.26 0 12 0zm0 2.16c3.203 0 3.585.016 4.85.071 1.17.055 1.805.249 2.227.415.562.217.96.477 1.382.896.419.42.679.819.896 1.381.164.422.36 1.057.413 2.227.057 1.266.07 1.646.07 4.85s-.015 3.585-.074 4.85c-.061 1.17-.256 1.805-.421 2.227-.224.562-.479.96-.899 1.382-.419.419-.824.679-1.38.896-.42.164-1.065.36-2.235.413-1.274.057-1.649.07-4.859.07-3.211 0-3.586-.015-4.859-.074-1.171-.061-1.816-.256-2.236-.421-.569-.224-.96-.479-1.379-.899-.421-.419-.69-.824-.9-1.38-.165-.42-.359-1.065-.42-2.235-.045-1.26-.061-1.649-.061-4.844 0-3.196.016-3.586.061-4.861.061-1.17.255-1.814.42-2.234.21-.57.479-.96.9-1.381.419-.419.81-.689 1.379-.898.42-.166 1.051-.361 2.221-.421 1.275-.045 1.65-.06 4.859-.06l.045.03zm0 3.678c-3.405 0-6.162 2.76-6.162 6.162 0 3.405 2.76 6.162 6.162 6.162 3.405 0 6.162-2.76 6.162-6.162 0-3.405-2.76-6.162-6.162-6.162zM12 16c-2.21 0-4-1.79-4-4s1.79-4 4-4 4 1.79 4 4-1.79 4-4 4zm7.846-10.405c0 .795-.646 1.44-1.44 1.44-.795 0-1.44-.646-1.44-1.44 0-.794.646-1.439 1.44-1.439.793-.001 1.44.645 1.44 1.439z"/></svg>
      </a>
      

      
      <a class="ml-0 footer-link icon" href="https://stackoverflow.com/story/tobiasreithmeier" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="StackOverflow">
        <svg class="stackoverflow svh-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>Stack Overflow</title><path d="M15.725 0l-1.72 1.277 6.39 8.588 1.716-1.277L15.725 0zm-3.94 3.418l-1.369 1.644 8.225 6.85 1.369-1.644-8.225-6.85zm-3.15 4.465l-.905 1.94 9.702 4.517.904-1.94-9.701-4.517zm-1.85 4.86l-.44 2.093 10.473 2.201.44-2.092-10.473-2.203zM1.89 15.47V24h19.19v-8.53h-2.133v6.397H4.021v-6.396H1.89zm4.265 2.133v2.13h10.66v-2.13H6.154Z"/></svg>
      </a>
      

    </div>
  
</div>

        </div>
      </div>

    </div>

  </div>
  <script src="/js/nanobar.min.js"></script>
  <script>
    var options = {
      classname: 'nanobar',
      id: 'myNanobar'
    };
    var nanobar = new Nanobar(options);
    nanobar.go(30);
    nanobar.go(76);
    nanobar.go(100);
  </script>

</body>

</html>
